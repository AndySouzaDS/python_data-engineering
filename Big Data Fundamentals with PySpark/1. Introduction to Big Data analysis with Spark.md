## # What is Big Data?
`Big Data` study and application of datasets that are too complex for traditional data processing sotfware

`Big Data (3 Vs)`
- Volume: Size of the data
- Variety: Different sources and formats
- Velocity: Speed of the data
### # Big Data concepts and Terminology

- `Clustered computing:` Collection of resources of multiple machines
- `Parallel computing:` Simultaneous computation
- `Distributed computing:` Collection of nodes (networked computers) that run in parallel
- `Batch processing:` Breaking the job into small pieces and running them on individual
machines
- `Real-time processing:` Immediate processing of data
### # Big Data processing systems

- Hadoop/MapReduce: Scalable and fault tolerant framework written in Java
  - Open source
  - Batch processing
- Apache Spark: General purpose and lightning fast cluster computing system
  - Open source
  - Both batch and real-time data processing
### # Features of Apache Spark framework
- Distributed cluster computing framework
- Efficient in-memory computations for large data sets
- Lightning fast data processing framework
- Provides support for Java, Scala, Python, R and SQL
### # Spark modes of deployment
- `Local mode:` Single machine such as your laptop
  - Local model convenient for testing, debugging and demonstration
- `Cluster mode:` Set of pre-defined machines
  - Good for production
- Workflow: Local-> clusters
- No code change necessary
## he 3 V's of Big Data
> Which of the following is NOT considered as one of the three Vs of Big Data?

Answer the question
- [ ] Volume
- [ ] Velocity
- [x] Variation
- [ ] Variety
## # PySpark: Spark with Python
### # Spark shell?
`spark shell` 
- Interactive environment for running Spark jobs
- Helpful for fast interactive prototyping
- Spark's shells allow interacting with data on disk or in memory
- Three different Spark shells:
  - Spark-shell for Scala
  - PySpark-shell for Python
  - SparkR for R
## # Loading data in PySpark

- SparkContext's `parallelize ()` method
```py
rdd = sc.parallelize ([1, 2, 3,4,5])
```
- SparkContext's `textFile ()` method
```py
rdd2 sc.textFile ("test.txt")
```
## Understanding SparkContext

A SparkContext represents the entry point to Spark functionality. It's like a key to your car. When we run any Spark application, a driver program starts, which has the main function and your SparkContext gets initiated here. PySpark automatically creates a SparkContext for you in the PySpark shell (so you don't have to create it by yourself) and is exposed via a variable sc.

> In this simple exercise, you'll find out the attributes of the SparkContext in your PySpark shell which you'll be using for the rest of the course.

Instructions
- [x] Print the version of SparkContext in the PySpark shell.
- [x] Print the Python version of SparkContext in the PySpark shell.
- [x] What is the master of SparkContext in the PySpark shell?
```py
# Print the version of SparkContext
print("The version of Spark Context in the PySpark shell is", sc.version)

# Print the Python version of SparkContext
print("The Python version of Spark Context in the PySpark shell is", sc.pythonVer)

# Print the master of SparkContext
print("The master of Spark Context in the PySpark shell is", sc.master)

'''result:
The version of Spark Context in the PySpark shell is 3.2.0
The Python version of Spark Context in the PySpark shell is 3.9
The master of Spark Context in the PySpark shell is local[*]'''
```
## Interactive Use of PySpark

Spark comes with an interactive Python shell in which PySpark is already installed in it. PySpark shell is useful for basic testing and debugging and it is quite powerful. The easiest way to demonstrate the power of PySparkâ€™s shell is to start using it. In this example, you'll load a simple list containing numbers ranging from 1 to 100 in the PySpark shell.

The most important thing to understand here is that we are not creating any SparkContext object because PySpark automatically creates the SparkContext object named sc, by default in the PySpark shell.

Instructions
- [x] Create a Python list named numb containing the numbers 1 to 100.
- [x] Load the list into Spark using Spark Context's parallelize method and assign it to a variable spark_data.
```py
# Create a Python list of numbers from 1 to 100 
numb = range(1, 101)

# Load the list into PySpark  
spark_data = sc.parallelize(numb)
```
## Loading data in PySpark shell

In PySpark, we express our computation through operations on distributed collections that are automatically parallelized across the cluster. In the previous exercise, you have seen an example of loading a list as parallelized collections and in this exercise, you'll load the data from a local file in PySpark shell.

Remember you already have a SparkContext sc and file_path variable (which is the path to the README.md file) already available in your workspace.

Instructions
- [x] Load a local text file README.md in PySpark shell.
```py
# Load a local file into PySpark shell
lines = sc.textFile(file_path)
```
