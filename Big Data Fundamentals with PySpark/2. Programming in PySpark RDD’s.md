## # Abstracting Data with RDDs
`RDD` Resilient Distributed Datasets

  - `Resilient` Ability to withstand failures

  - `Distributed` Spanning across multiple machines
### # Creating RDDs. How to do it?

- Parallelizing an existing collection of objects
- External datasets:
  - Files in HDFS
  - Objects in Amazon $3 bucket
  - lines in a text file
- From existing RDDs
### # Parallelized collection (parallelizing)
- `parallelize ()` for creating RDDs from python lists
```py
numRDD = sc.parallelize ([1,2,3,4])
hellORDD = sc.parallelize ("Hello world")
type (helloRDD)
```
    <class 'pyspark.rdd.PipelinedRDD'>
    
### #From external datasets
```py
textFile () for creating RDDs from external datasets
fileRDD = sc.textFile ("README.md")
type(tileROD)
```
    class 'pyspark.rdd.PipelinedRDD">
### # Partitioning pyspark

- A partition is a logical division of a large distributed data set

`parallelize ()` method
```py
numRODsc.parallelize (range (18), minPartitions = 6)
```
`textFile()` method
```py
TileRDD= sc.textFile ("README.md", minPartitions 6)
```
- The number of partitions in an RDD can be found by using `getNunPartitions ()` method
