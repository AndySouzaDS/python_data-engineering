## # Abstracting Data with RDDs
`RDD` Resilient Distributed Datasets

  - `Resilient` Ability to withstand failures

  - `Distributed` Spanning across multiple machines
### # Creating RDDs. How to do it?

- Parallelizing an existing collection of objects
- External datasets:
  - Files in HDFS
  - Objects in Amazon $3 bucket
  - lines in a text file
- From existing RDDs
### # Parallelized collection (parallelizing)
- `parallelize ()` for creating RDDs from python lists
```py
numRDD = sc.parallelize ([1,2,3,4])
hellORDD = sc.parallelize ("Hello world")
type (helloRDD)
```
    <class 'pyspark.rdd.PipelinedRDD'>
    
### #From external datasets
```py
textFile () for creating RDDs from external datasets
fileRDD = sc.textFile ("README.md")
type(tileROD)
```
    class 'pyspark.rdd.PipelinedRDD">
### # Partitioning pyspark

- A partition is a logical division of a large distributed data set

`parallelize ()` method
```py
numRODsc.parallelize (range (18), minPartitions = 6)
```
`textFile()` method
```py
TileRDD= sc.textFile ("README.md", minPartitions 6)
```
- The number of partitions in an RDD can be found by using `getNunmPartitions ()` method
## RDDs from Parallelized collections

Resilient Distributed Dataset (RDD) is the basic abstraction in Spark. It is an immutable distributed collection of objects. Since RDD is a fundamental and backbone data type in Spark, it is important that you understand how to create it. In this exercise, you'll create your first RDD in PySpark from a collection of words.

Remember you already have a SparkContext sc available in your workspace.

Instructions
- [x] Create an RDD named RDD from a list of words.
- [x] Confirm the object created is RDD.
```py
# Create an RDD from a list of words
RDD = sc.parallelize(["Spark", "is", "a", "framework", "for", "Big Data processing"])

# Print out the type of the created object
print("The type of RDD is", type(RDD))

# The type of RDD is <class 'pyspark.rdd.RDD'>
```
## RDDs from External Datasets

PySpark can easily create RDDs from files that are stored in external storage devices such as HDFS (Hadoop Distributed File System), Amazon S3 buckets, etc. However, the most common method of creating RDD's is from files stored in your local file system. This method takes a file path and reads it as a collection of lines. In this exercise, you'll create an RDD from the file path (file_path) with the file name README.md which is already available in your workspace.

Remember you already have a SparkContext sc available in your workspace.

Instructions
- [x] Print the file_path in the PySpark shell.
- [x] Create an RDD named fileRDD from a file_path.
- [x] Print the type of the fileRDD created.
```py
# Print the file_path
print("The file_path is", file_path)

# Create a fileRDD from file_path
fileRDD = sc.textFile(file_path)

# Check the type of fileRDD
print("The file type of fileRDD is", type(fileRDD))

'''result : 

The file_path is /usr/local/share/datasets/README.md
The file type of fileRDD is <class 'pyspark.rdd.RDD'>
'''
```
## Partitions in your data

SparkContext's textFile() method takes an optional second argument called minPartitions for specifying the minimum number of partitions. In this exercise, you'll create an RDD named fileRDD_part with 5 partitions and then compare that with fileRDD that you created in the previous exercise. Refer to the "Understanding Partition" slide in video 2.1 to know the methods for creating and getting the number of partitions in an RDD.

Remember, you already have a SparkContext sc, file_path and fileRDD available in your workspace.

Instructions
- [x] Find the number of partitions that support fileRDD RDD.
- [x] Create an RDD named fileRDD_part from the file path but create 5 partitions.
- [x] Confirm the number of partitions in the new fileRDD_part RDD.
```py
# Check the number of partitions in fileRDD
print("Number of partitions in fileRDD is", fileRDD.getNumPartitions())

# Create a fileRDD_part from file_path with 5 partitions
fileRDD_part = sc.textFile(file_path, minPartitions = 5)

# Check the number of partitions in fileRDD_part
print("Number of partitions in fileRDD_part is", fileRDD_part.getNumPartitions())

'''result : 

Number of partitions in fileRDD is 1
Number of partitions in fileRDD_part is 5
'''
```
