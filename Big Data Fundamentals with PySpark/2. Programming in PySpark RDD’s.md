## # Abstracting Data with RDDs
`RDD` Resilient Distributed Datasets

  - `Resilient` Ability to withstand failures

  - `Distributed` Spanning across multiple machines
### # Creating RDDs. How to do it?

- Parallelizing an existing collection of objects
- External datasets:
  - Files in HDFS
  - Objects in Amazon $3 bucket
  - lines in a text file
- From existing RDDs
### # Parallelized collection (parallelizing)
- `parallelize ()` for creating RDDs from python lists
```py
numRDD = sc.parallelize ([1,2,3,4])
hellORDD = sc.parallelize ("Hello world")
type (helloRDD)
```
    <class 'pyspark.rdd.PipelinedRDD'>
    
### #From external datasets
```py
textFile () for creating RDDs from external datasets
fileRDD = sc.textFile ("README.md")
type(tileROD)
```
    class 'pyspark.rdd.PipelinedRDD">
### # Partitioning pyspark

- A partition is a logical division of a large distributed data set

`parallelize ()` method
```py
numRODsc.parallelize (range (18), minPartitions = 6)
```
`textFile()` method
```py
TileRDD= sc.textFile ("README.md", minPartitions 6)
```
- The number of partitions in an RDD can be found by using `getNunmPartitions ()` method
## RDDs from Parallelized collections

Resilient Distributed Dataset (RDD) is the basic abstraction in Spark. It is an immutable distributed collection of objects. Since RDD is a fundamental and backbone data type in Spark, it is important that you understand how to create it. In this exercise, you'll create your first RDD in PySpark from a collection of words.

Remember you already have a SparkContext sc available in your workspace.

Instructions
- [x] Create an RDD named RDD from a list of words.
- [x] Confirm the object created is RDD.
```py
# Create an RDD from a list of words
RDD = sc.parallelize(["Spark", "is", "a", "framework", "for", "Big Data processing"])

# Print out the type of the created object
print("The type of RDD is", type(RDD))

# The type of RDD is <class 'pyspark.rdd.RDD'>
```
## RDDs from External Datasets

PySpark can easily create RDDs from files that are stored in external storage devices such as HDFS (Hadoop Distributed File System), Amazon S3 buckets, etc. However, the most common method of creating RDD's is from files stored in your local file system. This method takes a file path and reads it as a collection of lines. In this exercise, you'll create an RDD from the file path (file_path) with the file name README.md which is already available in your workspace.

Remember you already have a SparkContext sc available in your workspace.

Instructions
- [x] Print the file_path in the PySpark shell.
- [x] Create an RDD named fileRDD from a file_path.
- [x] Print the type of the fileRDD created.
```py
# Print the file_path
print("The file_path is", file_path)

# Create a fileRDD from file_path
fileRDD = sc.textFile(file_path)

# Check the type of fileRDD
print("The file type of fileRDD is", type(fileRDD))

'''result : 

The file_path is /usr/local/share/datasets/README.md
The file type of fileRDD is <class 'pyspark.rdd.RDD'>
'''
```
## Partitions in your data

SparkContext's textFile() method takes an optional second argument called minPartitions for specifying the minimum number of partitions. In this exercise, you'll create an RDD named fileRDD_part with 5 partitions and then compare that with fileRDD that you created in the previous exercise. Refer to the "Understanding Partition" slide in video 2.1 to know the methods for creating and getting the number of partitions in an RDD.

Remember, you already have a SparkContext sc, file_path and fileRDD available in your workspace.

Instructions
- [x] Find the number of partitions that support fileRDD RDD.
- [x] Create an RDD named fileRDD_part from the file path but create 5 partitions.
- [x] Confirm the number of partitions in the new fileRDD_part RDD.
```py
# Check the number of partitions in fileRDD
print("Number of partitions in fileRDD is", fileRDD.getNumPartitions())

# Create a fileRDD_part from file_path with 5 partitions
fileRDD_part = sc.textFile(file_path, minPartitions = 5)

# Check the number of partitions in fileRDD_part
print("Number of partitions in fileRDD_part is", fileRDD_part.getNumPartitions())

'''result : 

Number of partitions in fileRDD is 1
Number of partitions in fileRDD_part is 5
'''
```
## # Basic RDD Transformations and Actions
### # Pyspark operations 
- `transformations` : creation of new RDDs
      
  - Basic RDD Transformations
      `map ()`, `filter ()`, `flat Map ()`, and `union()`
```py
''' map () example : returns squared numbers'''
RDD = sc.parallelize ( [1,2,3,4])
RDD map RDD.map (Lambda x: x * x)

''' filter () example : returns bigger than 2 '''
RDD = SC.parallelize ( [1,2,3,4])
RDD_filter = RDD.filter (Lambda x: x > 2)

''' flatMap () transformation example: retrun multiple values for each element splitted '''
RDD = sc.parallelize (["hello world", "how are you")
RDD flatmap = RDD.flatMap (Lambda x: x.split (" "))

''' union () example : return warnings and errors as badlines or combined '''
inputRDD = sc.textFile ("Logs.txt")
errorRDD= inputRDD.filter (lambda x: "error" in x.split())
warningsRDD inputRDD.filter (lambda x: "warnings" in x.split())
combinedRDD = errorRDD.union(warningsRDD)
```
- `actions` : perform computation on RDDs

  - Basic RDD Actions
```py
collect()
take(N)
first()
count()
```
## Map and Collect

The main method by which you can manipulate data in PySpark is using map(). The map() transformation takes in a function and applies it to each element in the RDD. It can be used to do any number of things, from fetching the website associated with each URL in our collection to just squaring the numbers. In this simple exercise, you'll use map() transformation to cube each number of the numbRDD RDD that you created earlier. Next, you'll return all the elements to a variable and finally print the output.

Remember, you already have a SparkContext sc, and numbRDD available in your workspace.

Instructions
- [x] Create map() transformation that cubes all of the numbers in numbRDD.
- [x] Collect the results in a numbers_all variable.
- [x] Print the output from numbers_all variable.
```py
# Create map() transformation to cube numbers
cubedRDD = numbRDD.map(lambda x: x**3)

# Collect the results
numbers_all = cubedRDD.collect()

# Print the numbers from numbers_all
for numb in numbers_all:
	print(numb)

''' result :
1
8
27
64
125
216
343
512
729
1000
'''
```
## Filter and Count

The RDD transformation filter() returns a new RDD containing only the elements that satisfy a particular function. It is useful for filtering large datasets based on a keyword. For this exercise, you'll filter out lines containing keyword Spark from fileRDD RDD which consists of lines of text from the README.md file. Next, you'll count the total number of lines containing the keyword Spark and finally print the first 4 lines of the filtered RDD.

Remember, you already have a SparkContext sc, file_path and fileRDD available in your workspace.

Instructions
- [x] Create filter() transformation to select the lines containing the keyword Spark.
- [x] How many lines in fileRDD_filter contains the keyword Spark?
- [x] Print the first four lines of the resulting RDD.
```py
# Filter the fileRDD to select lines with Spark keyword
fileRDD_filter = fileRDD.filter(lambda line: 'Spark' in line)

# How many lines are there in fileRDD?
print("The total number of lines with the keyword Spark is", fileRDD_filter.count())

# Print the first four lines of fileRDD
for line in fileRDD_filter.take(4): 
  print(line)

''' result : 

The total number of lines with the keyword Spark is 7
Examples for Learning Spark
Examples for the Learning Spark book. These examples require a number of libraries and as such have long build files. We have also added a stand alone example with minimal dependencies and a small build file
These examples have been updated to run against Spark 1.3 so they may
be slightly different than the versions in your copy of "Learning Spark".
'''
```
## # Pair RDDs in PySpark
### # Creating pair RDDs
- From a list of key-value tuple
- From a regular RDD
```py
my_tuple= [('Sam", 23), (Mary', 34), (Peter, 25)]
pairRDD_tuple = sc.parallelize (my_tuple)


my_list = ['Sam 23, Mary 34", 'Peter 25']
regularRDD = sc.parallelize (my_list)
pairRDD_RDD = regularRDD.map (Lambda s: (s.split (' ') tol, s.split (' ')[1]))
```
### # Examples of paired RDD Transformations
- `reduceByKey(func)`: Combine values with the same key
- `groupByKey()`: Group values with the same key
- `sortByKey()`: Return an RDD sorted by the key
- `join()`: Join two pair RDDs based on their !
## ReduceBykey and Collect

One of the most popular pair RDD transformations is reduceByKey() which operates on key, value (k,v) pairs and merges the values for each key. In this exercise, you'll first create a pair RDD from a list of tuples, then combine the values with the same key and finally print out the result.

Remember, you already have a SparkContext sc available in your workspace.

Instructions
- [x] Create a pair RDD named Rdd with tuples (1,2),(3,4),(3,6),(4,5).
- [x] Transform the Rdd with reduceByKey() into a pair RDD Rdd_Reduced by adding the values with the same key.
- [x] Collect the contents of pair RDD Rdd_Reduced and iterate to print the output.
```py
# Create PairRDD Rdd with key value pairs
Rdd = sc.parallelize([(1,2), (3,4), (3,6), (4,5)])

# Apply reduceByKey() operation on Rdd
Rdd_Reduced = Rdd.reduceByKey(lambda x, y: x + y)

# Iterate over the result and print the output
for num in Rdd_Reduced.collect(): 
  print("Key {} has {} Counts".format(num[0], num[1]))
```
## SortByKey and Collect

Many times it is useful to sort the pair RDD based on the key (for example word count which you'll see later in the chapter). In this exercise, you'll sort the pair RDD Rdd_Reduced that you created in the previous exercise into descending order and print the final output.

Remember, you already have a SparkContext sc and Rdd_Reduced available in your workspace.

Instructions
- [x] Sort the Rdd_Reduced RDD using the key in descending order.
- [x] Collect the contents and iterate to print the output.
```py
# Sort the reduced RDD with the key by descending order
Rdd_Reduced_Sort = Rdd_Reduced.sortByKey(ascending=False)

# Iterate over the result and retrieve all the elements of the RDD
for num in Rdd_Reduced_Sort.collect():
  print("Key {} has {} Counts".format(num[0], num[1]))

''' result :

Key 4 has 5 Counts
Key 3 has 10 Counts
Key 1 has 2 Counts
'''
```
