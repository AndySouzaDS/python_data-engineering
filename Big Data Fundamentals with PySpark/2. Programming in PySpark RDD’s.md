## # Abstracting Data with RDDs
`RDD` Resilient Distributed Datasets

  - `Resilient` Ability to withstand failures

  - `Distributed` Spanning across multiple machines
### # Creating RDDs. How to do it?

- Parallelizing an existing collection of objects
- External datasets:
  - Files in HDFS
  - Objects in Amazon $3 bucket
  - lines in a text file
- From existing RDDs
### # Parallelized collection (parallelizing)
- `parallelize ()` for creating RDDs from python lists
```py
numRDD = sc.parallelize ([1,2,3,4])
hellORDD = sc.parallelize ("Hello world")
type (helloRDD)
```
    <class 'pyspark.rdd.PipelinedRDD'>
    
### #From external datasets
```py
textFile () for creating RDDs from external datasets
fileRDD = sc.textFile ("README.md")
type(tileROD)
```
    class 'pyspark.rdd.PipelinedRDD">
### # Partitioning pyspark

- A partition is a logical division of a large distributed data set

`parallelize ()` method
```py
numRODsc.parallelize (range (18), minPartitions = 6)
```
`textFile()` method
```py
TileRDD= sc.textFile ("README.md", minPartitions 6)
```
- The number of partitions in an RDD can be found by using `getNunmPartitions ()` method
## RDDs from Parallelized collections

Resilient Distributed Dataset (RDD) is the basic abstraction in Spark. It is an immutable distributed collection of objects. Since RDD is a fundamental and backbone data type in Spark, it is important that you understand how to create it. In this exercise, you'll create your first RDD in PySpark from a collection of words.

Remember you already have a SparkContext sc available in your workspace.

Instructions
- [x] Create an RDD named RDD from a list of words.
- [x] Confirm the object created is RDD.
```py
# Create an RDD from a list of words
RDD = sc.parallelize(["Spark", "is", "a", "framework", "for", "Big Data processing"])

# Print out the type of the created object
print("The type of RDD is", type(RDD))

# The type of RDD is <class 'pyspark.rdd.RDD'>
```
## RDDs from External Datasets

PySpark can easily create RDDs from files that are stored in external storage devices such as HDFS (Hadoop Distributed File System), Amazon S3 buckets, etc. However, the most common method of creating RDD's is from files stored in your local file system. This method takes a file path and reads it as a collection of lines. In this exercise, you'll create an RDD from the file path (file_path) with the file name README.md which is already available in your workspace.

Remember you already have a SparkContext sc available in your workspace.

Instructions
- [x] Print the file_path in the PySpark shell.
- [x] Create an RDD named fileRDD from a file_path.
- [x] Print the type of the fileRDD created.
```py
# Print the file_path
print("The file_path is", file_path)

# Create a fileRDD from file_path
fileRDD = sc.textFile(file_path)

# Check the type of fileRDD
print("The file type of fileRDD is", type(fileRDD))

'''result : 

The file_path is /usr/local/share/datasets/README.md
The file type of fileRDD is <class 'pyspark.rdd.RDD'>
'''
```
## Partitions in your data

SparkContext's textFile() method takes an optional second argument called minPartitions for specifying the minimum number of partitions. In this exercise, you'll create an RDD named fileRDD_part with 5 partitions and then compare that with fileRDD that you created in the previous exercise. Refer to the "Understanding Partition" slide in video 2.1 to know the methods for creating and getting the number of partitions in an RDD.

Remember, you already have a SparkContext sc, file_path and fileRDD available in your workspace.

Instructions
- [x] Find the number of partitions that support fileRDD RDD.
- [x] Create an RDD named fileRDD_part from the file path but create 5 partitions.
- [x] Confirm the number of partitions in the new fileRDD_part RDD.
```py
# Check the number of partitions in fileRDD
print("Number of partitions in fileRDD is", fileRDD.getNumPartitions())

# Create a fileRDD_part from file_path with 5 partitions
fileRDD_part = sc.textFile(file_path, minPartitions = 5)

# Check the number of partitions in fileRDD_part
print("Number of partitions in fileRDD_part is", fileRDD_part.getNumPartitions())

'''result : 

Number of partitions in fileRDD is 1
Number of partitions in fileRDD_part is 5
'''
```
## # Basic RDD Transformations and Actions
### # Pyspark operations 
- `transformations` : creation of new RDDs
      
  - Basic RDD Transformations
      `map ()`, `filter ()`, `flat Map ()`, and `union()`
```py
''' map () example : returns squared numbers'''
RDD = sc.parallelize ( [1,2,3,4])
RDD map RDD.map (Lambda x: x * x)

''' filter () example : returns bigger than 2 '''
RDD = SC.parallelize ( [1,2,3,4])
RDD_filter = RDD.filter (Lambda x: x > 2)

''' flatMap () transformation example: retrun multiple values for each element splitted '''
RDD = sc.parallelize (["hello world", "how are you")
RDD flatmap = RDD.flatMap (Lambda x: x.split (" "))

''' union () example : return warnings and errors as badlines or combined '''
inputRDD = sc.textFile ("Logs.txt")
errorRDD= inputRDD.filter (lambda x: "error" in x.split())
warningsRDD inputRDD.filter (lambda x: "warnings" in x.split())
combinedRDD = errorRDD.union(warningsRDD)
```
- `actions` : perform computation on RDDs

  - Basic RDD Actions
```py
collect()
take(N)
first()
count()
```
## Map and Collect

The main method by which you can manipulate data in PySpark is using map(). The map() transformation takes in a function and applies it to each element in the RDD. It can be used to do any number of things, from fetching the website associated with each URL in our collection to just squaring the numbers. In this simple exercise, you'll use map() transformation to cube each number of the numbRDD RDD that you created earlier. Next, you'll return all the elements to a variable and finally print the output.

Remember, you already have a SparkContext sc, and numbRDD available in your workspace.

Instructions
- [x] Create map() transformation that cubes all of the numbers in numbRDD.
- [x] Collect the results in a numbers_all variable.
- [x] Print the output from numbers_all variable.
```py
# Create map() transformation to cube numbers
cubedRDD = numbRDD.map(lambda x: x**3)

# Collect the results
numbers_all = cubedRDD.collect()

# Print the numbers from numbers_all
for numb in numbers_all:
	print(numb)

''' result :
1
8
27
64
125
216
343
512
729
1000
'''
```
## Filter and Count

The RDD transformation filter() returns a new RDD containing only the elements that satisfy a particular function. It is useful for filtering large datasets based on a keyword. For this exercise, you'll filter out lines containing keyword Spark from fileRDD RDD which consists of lines of text from the README.md file. Next, you'll count the total number of lines containing the keyword Spark and finally print the first 4 lines of the filtered RDD.

Remember, you already have a SparkContext sc, file_path and fileRDD available in your workspace.

Instructions
- [x] Create filter() transformation to select the lines containing the keyword Spark.
- [x] How many lines in fileRDD_filter contains the keyword Spark?
- [x] Print the first four lines of the resulting RDD.
```py
# Filter the fileRDD to select lines with Spark keyword
fileRDD_filter = fileRDD.filter(lambda line: 'Spark' in line)

# How many lines are there in fileRDD?
print("The total number of lines with the keyword Spark is", fileRDD_filter.count())

# Print the first four lines of fileRDD
for line in fileRDD_filter.take(4): 
  print(line)

''' result : 

The total number of lines with the keyword Spark is 7
Examples for Learning Spark
Examples for the Learning Spark book. These examples require a number of libraries and as such have long build files. We have also added a stand alone example with minimal dependencies and a small build file
These examples have been updated to run against Spark 1.3 so they may
be slightly different than the versions in your copy of "Learning Spark".
'''
```
## # Pair RDDs in PySpark
### # Creating pair RDDs
- From a list of key-value tuple
- From a regular RDD
```py
my_tuple= [('Sam", 23), (Mary', 34), (Peter, 25)]
pairRDD_tuple = sc.parallelize (my_tuple)


my_list = ['Sam 23, Mary 34", 'Peter 25']
regularRDD = sc.parallelize (my_list)
pairRDD_RDD = regularRDD.map (Lambda s: (s.split (' ') tol, s.split (' ')[1]))
```
### # Examples of paired RDD Transformations
- `reduceByKey(func)`: Combine values with the same key
- `groupByKey()`: Group values with the same key
- `sortByKey()`: Return an RDD sorted by the key
- `join()`: Join two pair RDDs based on their !
## ReduceBykey and Collect

One of the most popular pair RDD transformations is reduceByKey() which operates on key, value (k,v) pairs and merges the values for each key. In this exercise, you'll first create a pair RDD from a list of tuples, then combine the values with the same key and finally print out the result.

Remember, you already have a SparkContext sc available in your workspace.

Instructions
- [x] Create a pair RDD named Rdd with tuples (1,2),(3,4),(3,6),(4,5).
- [x] Transform the Rdd with reduceByKey() into a pair RDD Rdd_Reduced by adding the values with the same key.
- [x] Collect the contents of pair RDD Rdd_Reduced and iterate to print the output.
```py
# Create PairRDD Rdd with key value pairs
Rdd = sc.parallelize([(1,2), (3,4), (3,6), (4,5)])

# Apply reduceByKey() operation on Rdd
Rdd_Reduced = Rdd.reduceByKey(lambda x, y: x + y)

# Iterate over the result and print the output
for num in Rdd_Reduced.collect(): 
  print("Key {} has {} Counts".format(num[0], num[1]))
```
## SortByKey and Collect

Many times it is useful to sort the pair RDD based on the key (for example word count which you'll see later in the chapter). In this exercise, you'll sort the pair RDD Rdd_Reduced that you created in the previous exercise into descending order and print the final output.

Remember, you already have a SparkContext sc and Rdd_Reduced available in your workspace.

Instructions
- [x] Sort the Rdd_Reduced RDD using the key in descending order.
- [x] Collect the contents and iterate to print the output.
```py
# Sort the reduced RDD with the key by descending order
Rdd_Reduced_Sort = Rdd_Reduced.sortByKey(ascending=False)

# Iterate over the result and retrieve all the elements of the RDD
for num in Rdd_Reduced_Sort.collect():
  print("Key {} has {} Counts".format(num[0], num[1]))

''' result :

Key 4 has 5 Counts
Key 3 has 10 Counts
Key 1 has 2 Counts
'''
```
## # Advanced RDD Actions
`reduce ()` action. Used to aggregate elements of regular RDD
```py
x = [1,3,4,6]
RDD SC.parallelize (x)
''' aggregate Xs in list '''
RDD.reduce (Lambda x, y: x + y)

# result : 14
```
`saveAsTextFile ()` action saves RDD into a text file inside a directory with each partition as
a separate file
```py
RDD.saveAsTextFile("tempFile")
```
`coalesce ()` method can be used to save RDD as a single text file
```py
RDD.coalesce(1).saveAsTextFile("tempFile")
```
`countByKey ()` counts the number of elements for each key
- only available for type (K, V)
```py
rdd = sc.parallelize ([("a", 1), ("b", 1), ("a", 1)))
for kee, val in rdd.countByKey().items():
print (kee, val)
```
	('a', 2)
	('b', 1)
`collectAsMap ()` return the key-value pairs in the RDD as a dictionary
```py
sc.parallelize ([(1, 2), (3, 4)]).collectAsMap()
```
	{1: 2, 3: 4}
## CountingBykeys

For many datasets, it is important to count the number of keys in a key/value dataset. For example, counting the number of countries where the product was sold or to show the most popular baby names. In this simple exercise, you'll use the Rdd that you created earlier and count the number of unique keys in that pair RDD.

Remember, you already have a SparkContext sc and Rdd available in your workspace.

Instructions
- [x] Count the unique keys and assign the result to a variable total.
- [x] What is the type of total?
- [x] Iterate over the total and print the keys and their counts.
```py
# Count the unique keys
total = Rdd.countByKey()

# What is the type of total?
print("The type of total is", type(total))

# Iterate over the total and print the output
for k, v in total.items(): 
  print("key", k, "has", v, "counts")

''' result :

The type of total is <class 'collections.defaultdict'>
key 1 has 1 counts
key 3 has 2 counts
key 4 has 1 counts
'''
```
## Create a base RDD and transform it

The volume of unstructured data (log lines, images, binary files) in existence is growing dramatically, and PySpark is an excellent framework for analyzing this type of data through RDDs. In this 3 part exercise, you will write code that calculates the most common words from Complete Works of William Shakespeare.

Here are the brief steps for writing the word counting program:

Create a base RDD from Complete_Shakespeare.txt file.
Use RDD transformation to create a long list of words from each element of the base RDD.
Remove stop words from your data.
Create pair RDD where each element is a pair tuple of ('w', 1)
Group the elements of the pair RDD by key (word) and add up their values.
Swap the keys (word) and values (counts) so that keys is count and value is the word.
Finally, sort the RDD by descending order and print the 10 most frequent words and their frequencies.
In this first exercise, you'll create a base RDD from Complete_Shakespeare.txt file and transform it to create a long list of words.

Remember, you already have a SparkContext sc already available in your workspace. A file_path variable (which is the path to the Complete_Shakespeare.txt file) is also loaded for you.

Instructions
- [x] Create an RDD called baseRDD that reads lines from file_path.
- [x] Transform the baseRDD into a long list of words and create a new splitRDD.
- [x] Count the total words in splitRDD.
```py
# Create a baseRDD from the file path
baseRDD = sc.textFile(file_path)

# Split the lines of baseRDD into words
splitRDD = baseRDD.flatMap(lambda x: x.split())

# Count the total number of words
print("Total number of words in splitRDD:", splitRDD.count())

# Total number of words in splitRDD: 904061
```
## Remove stop words and reduce the dataset

- [x] Convert the words in splitRDD in lower case and then remove stop words from stop_words curated list.
- [x] Create a pair RDD tuple containing the word and the number 1 from each word element in splitRDD.
- [x] Get the count of the number of occurrences of each word (word frequency) in the pair RDD.
```py
# Convert the words in lower case and remove stop words from the stop_words curated list
splitRDD_no_stop = splitRDD.filter(lambda x: x.lower() not in stop_words)

# Create a tuple of the word and 1 
splitRDD_no_stop_words = splitRDD_no_stop.map(lambda w: (w, 1))

# Count of the number of occurences of each word
resultRDD = splitRDD_no_stop_words.reduceByKey(lambda x, y: x + y)
```
## print word frequencies 
- [x] Print the first 10 words and their frequencies from the resultRDD RDD.
- [x] Swap the keys and values in the resultRDD.
- [x] Sort the keys according to descending order.
- [x] Print the top 10 most frequent words and their frequencies from the sorted RDD
```py
# Display the first 10 words and their frequencies from the input RDD
for word in resultRDD.take(10):
	print(word)

# Swap the keys and values from the input RDD
resultRDD_swap = resultRDD.map(lambda x: (x[1], x[0]))

# Sort the keys in descending order
resultRDD_swap_sort = resultRDD_swap.sortByKey(ascending=False)

# Show the top 10 most frequent words and their frequencies from the sorted RDD
for word in resultRDD_swap_sort.take(10):
	print("{},{}". format(word[1], word[0]))
```
