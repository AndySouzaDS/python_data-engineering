## # Abstracting Data with DataFrames
### # create df from RDD
```py
iphones_R00 sc.parallelize ([
  (XS, 2018, 5.65, 2.79, 6.24),
  (XR", 2018, 5.94, 2.98, 6.84),
  (X10, 2017, 5.65, 2.79, 6.13),
  (8PLUS, 2017, 6.23, 3.07, 7.12)
])

names ["Model", "Year", "Height", "Width", "Weight"]

iphones_df = spark.createDataFrame (iphones_RDD, schema=names)
type(iphones_df)
```
    pyspark. sql.dataframe . DataFrame
### # Create a DataFrame from reading a CSV/JSON/TXT
```py
df_csv spark. read.csv("people. csv", header=True , inferSchema=True)
df_json spark.read.json("people . json", header=True, inferSchema=True)
df txt= spark. read. txt("people. txt", header=True, inferSchema= True)
```
- Path to the file and two optional parameters
- Two optional parameters
  `header=True, inferSchema=True`
## RDD to DataFrame
Instructions
- [x] Create an RDD from the sample_list.
- [x] Create a PySpark DataFrame using the above RDD and schema.
- [x] Confirm the output as PySpark DataFrame.
```py
# Create a list of tuples
sample_list = [('Mona',20), ('Jennifer',34), ('John',20), ('Jim',26)]

# Create an RDD from the list
rdd = sc.parallelize(sample_list)

# Create a PySpark DataFrame
names_df = spark.createDataFrame(rdd, schema=['Name', 'Age'])

# Check the type of names_df
print("The type of names_df is", type(names_df))

# The type of names_df is <class 'pyspark.sql.dataframe.DataFrame'>
```
## Loading CSV into DataFrame
Instructions
- [x] Create a DataFrame from file_path variable which is the path to the people.csv file.
- [x] Confirm the output as PySpark DataFrame.
```py
# Create an DataFrame from file_path # people.csv
people_df = spark.read.csv(file_path, header=True, inferSchema=True)

# Check the type of people_df
print("The type of people_df is", type(people_df))

# The type of people_df is <class 'pyspark.sql.dataframe.DataFrame'>
```
##  Inspecting data in PySpark DataFrame
Instructions
- [x] Print the first 10 observations in the people_df DataFrame.
- [x] Count the number of rows in the people_df DataFrame.
- [x] How many columns does people_df DataFrame have and what are their names?
```py
# Print the first 10 observations 
people_df.show(10)

# Count the number of rows 
print("There are {} rows in the people_df DataFrame.".format(people_df.count()))

# Count the number of columns and their names
print("There are {} columns in the people_df DataFrame and their names are {}".format(len(people_df.columns), people_df.columns))

''' result :

There are 100000 rows in the people_df DataFrame.
There are 5 columns in the people_df DataFrame and their names are ['_c0', 'person_id', 'name', 'sex', 'date of birth']
'''
```
## PySpark DataFrame subsetting and cleaning
- [x] Select 'name', 'sex' and 'date of birth' columns from people_df and create people_df_sub DataFrame.
- [x] Print the first 10 observations in the people_df DataFrame.
- [x] Remove duplicate entries from people_df_sub DataFrame and create people_df_sub_nodup DataFrame.
- [x] How many rows are there before and after duplicates are removed?
```py
# Select name, sex and date of birth columns
people_df_sub = people_df.select('name', 'sex', 'date of birth')

# Print the first 10 observations from people_df_sub
people_df_sub.show(10)

# Remove duplicate entries from people_df_sub
people_df_sub_nodup = people_df_sub.dropDuplicates()

# Count the number of rows
print("There were {} rows before removing duplicates, and {} rows after removing duplicates".format(people_df_sub.count(), people_df_sub_nodup.count()))

# There were 100000 rows before removing duplicates, and 99998 rows after removing duplicates
```
