## PySpark ML libraries
> What kind of data structures does pyspark.mllib built-in library support in Spark?

Answer the question
- [ ] DataFrames
- [x] RDDs
- [ ] Datasets
- [ ] All
## PySpark MLlib algorithms
- [x] Import pyspark.mllib recommendation submodule and Alternating Least Squares class.
- [x] Import pyspark.mllib classification submodule and Logistic Regression with LBFGS class.
- [x] Import pyspark.mllib clustering submodule and kmeans class.
## Loading Movie Lens dataset into RDDs
- [x] Load the ratings.csv dataset into an RDD.
- [x] Split the RDD using , as a delimiter.
- [x] For each line of the RDD, using Rating() class create a tuple of userID, productID, rating.
- [x] Randomly split the data into training data and test data (0.8 and 0.2).
```py
# Load the data into RDD
data = sc.textFile(file_path)

# Split the RDD 
ratings = data.map(lambda l: l.split(','))

# Transform the ratings RDD 
ratings_final = ratings.map(lambda line: Rating(int(line[0]), int(line[1]), float(line[2])))

# Split the data into training and test
training_data, test_data = ratings_final.randomSplit([0.8, 0.2])
```
## Model training and predictions
- [x] Train ALS algorithm with training data and configured parameters (rank = 10 and iterations = 10).
- [x] Drop the rating column in the test data.
- [x] Test the model by predicting the rating from the test data.
- [x] Return a list of two rows of the predicted ratings.
```py
# Create the ALS model on the training data
model = ALS.train(training_data, rank=10, iterations=10)

# Drop the ratings column 
testdata_no_rating = test_data.map(lambda p: (p[0], p[1]))

# Predict the model  
predictions = model.predictAll(testdata_no_rating)

# Return the first 2 rows of the RDD
predictions.take(2)

'''result :
[Rating(user=390, product=667, rating=2.8155512039648505),
 Rating(user=140, product=5618, rating=3.5501714642493836)]
 '''
 ```
 ## Model evaluation using MSE
- [x] Organize ratings RDD to make ((user, product), rating).
- [x] Organize predictions RDD to make ((user, product), rating).
- [x] Join the prediction RDD with the ratings RDD.
- [x] Evaluate the model using MSE between original rating and predicted rating and print it.
```py
# Prepare ratings data
rates = ratings_final.map(lambda r: ((r[0], r[1]), r[2]))

# Prepare predictions data
preds = predictions.map(lambda r: ((r[0], r[1]), r[2]))

# Join the ratings data with predictions data
rates_and_preds = rates.join(preds)

# Calculate and print MSE
MSE = rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean()
print("Mean Squared Error of the model for the test data = {:.2f}".format(MSE))

# Mean Squared Error of the model for the test data = 1.39
```
## Loading spam and non-spam data
- [x] Create two RDDS, one for 'spam' and one for 'non-spam (ham)'.
- [x] Split each email in 'spam' and 'non-spam' RDDs into words.
- [x] Print the first element in the split RDD of both 'spam' and 'non-spam'.
```py
# Load the datasets into RDDs
spam_rdd = sc.textFile(file_path_spam)
non_spam_rdd = sc.textFile(file_path_non_spam)

# Split the email messages into words
spam_words = spam_rdd.flatMap(lambda email: email.split(' '))
non_spam_words = non_spam_rdd.flatMap(lambda email: email.split(' '))

# Print the first element in the split RDD
print("The first element in spam_words is", spam_words.first())
print("The first element in non_spam_words is", non_spam_words.first())

''' result : 

The first element in spam_words is ['You', 'have', '1', 'new', 'message.', 'Please', 'call', '08712400200.']
The first element in non_spam_words is ['Rofl.', 'Its', 'true', 'to', 'its', 'name']

'''
```
## Feature hashing and LabelPoint
- [x] Create a HashingTF() instance to map email text to vectors of 200 features.
- [x] Each message in 'spam' and 'non-spam' datasets are split into words, and each word is mapped to one feature.
- [x] Label the features: 1 for spam, 0 for non-spam.
- [x] Combine both the spam and non-spam samples into a single dataset.\
```py
