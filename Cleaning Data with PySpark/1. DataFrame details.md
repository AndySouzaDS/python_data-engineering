## # Intro to data cleaning with Apache Spark

### # data cleaning
`data cleaning` Preparing raw data for use in data processing pipelines.

`data cleaning tasks` 
- Reformatting or replacing text
- Performing calculations
- Removing garbage or incomplete data
- 
###  # why cleaning w/ SPARK ?
- Problems with typical data systems:
  - Performance
  - Organizing data flow
- ***Advantages of Spark:***
  - Scalable
  - Powerful framework for data handling
### # data cleaning example 
![image](https://user-images.githubusercontent.com/51888893/203651989-f3cce7f4-ce84-4b07-8c30-b02b684a7639.png)

### # Spark Schemas
`spark schemas` :
- Define format of DataFrame
- various data types:
  - Strings, dates, integers, arrays
- filter garbage data during import
- Improves read performance
- 
### # Example Spark Schema
- Import schema
```py
import pyspark.sql.types
peopleSchema = StructType ([
# Define the name field
StructField ( 'name', StringType(), True),
#Add the age field
StructField ('age' , IntegerType(), True),
# Add the city field
StructField ('city', StringType(), True)
])
```
- Read CSV file containing data
```py
people_df = spark. read .format (" csv' ) .load (name='rawdata .csv', schema=peopleSchema)
```
## Data cleaning review
> Which of the following is NOT a benefit?

Answer the question
- [ ] Spark offers high performance.
- [x] Spark can only handle thousands of records.
- [ ] Spark allows orderly data flows.
- [ ] Spark can use strictly defined schemas while ingesting data.
## Defining a schema
- [x] Import * from the pyspark.sql.types library.
- [x] Define a new schema using the StructType method.
- [x] Define a StructField for name, age, and city. Each field should correspond to the correct datatype and not be nullable.
