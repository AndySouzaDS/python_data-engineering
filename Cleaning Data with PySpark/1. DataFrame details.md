## # Intro to data cleaning with Apache Spark

### # data cleaning
`data cleaning` Preparing raw data for use in data processing pipelines.

`data cleaning tasks` 
- Reformatting or replacing text
- Performing calculations
- Removing garbage or incomplete data
- 
###  # why cleaning w/ SPARK ?
- Problems with typical data systems:
  - Performance
  - Organizing data flow
- ***Advantages of Spark:***
  - Scalable
  - Powerful framework for data handling
### # data cleaning example 
![image](https://user-images.githubusercontent.com/51888893/203651989-f3cce7f4-ce84-4b07-8c30-b02b684a7639.png)

### # Spark Schemas
`spark schemas` :
- Define format of DataFrame
- various data types:
  - Strings, dates, integers, arrays
- filter garbage data during import
- Improves read performance
- 
### # Example Spark Schema
- Import schema
```py
import pyspark.sql.types
peopleSchema = StructType ([
# Define the name field
StructField ( 'name', StringType(), True),
#Add the age field
StructField ('age' , IntegerType(), True),
# Add the city field
StructField ('city', StringType(), True)
])
```
- Read CSV file containing data
```py
people_df = spark. read .format (" csv' ) .load (name='rawdata .csv', schema=peopleSchema)
```
## Data cleaning review
> Which of the following is NOT a benefit?

Answer the question
- [ ] Spark offers high performance.
- [x] Spark can only handle thousands of records.
- [ ] Spark allows orderly data flows.
- [ ] Spark can use strictly defined schemas while ingesting data.
## Defining a schema
- [x] Import * from the pyspark.sql.types library.
- [x] Define a new schema using the StructType method.
- [x] Define a StructField for name, age, and city. Each field should correspond to the correct datatype and not be nullable.
```py
# Import the pyspark.sql.types library
from pyspark.sql.types import pyspark.sql.types.*

# Define a new schema using the StructType method
people_schema = StructType([
  # Define a StructField for each field
  StructField('name', StringType(), False), # False = not nullable
  StructField('age', IntegerType(), False),
  StructField('city', StringType(), False)
])
```
## Immutability review
> Youâ€™ve just seen that immutability and lazy processing are fundamental concepts in the way Spark handles data. But why would Spark use immutable data frames to begin with?

Answer the question
- [ ] To add complexity to your Spark tasks.
- [x] To efficiently handle data throughout the cluster.
- [ ] To easily modify variable values as needed.
- [ ] To conserve storage space.

## Using lazy processing
- [x] Load the Data Frame.
- [x] Add the transformation for F.lower() to the Destination Airport column.
- [x] Drop the Destination Airport column from the Data Frame aa_dfw_df. Note the time for these operations to complete.
- [x] Show the Data Frame, noting the time difference for this action to complete.
```py
# Load the CSV file
aa_dfw_df = spark.read.format('csv').options(Header=True).load('AA_DFW_2018.csv.gz')

# Add the airport column using the F.lower() method
aa_dfw_df = aa_dfw_df.withColumn('airport', F.lower(aa_dfw_df['Destination Airport']))

# Drop the Destination Airport column
aa_dfw_df = aa_dfw_df.drop(aa_dfw_df['Destination Airport'])

# Show the DataFrame
aa_dfw_df.show()
```
## # Understanding Parquet
### # Reading Parquet files
```py
df = spark.read.format('parquet').load('filename.parquet')

df = spark.read.parquet ('filename.parquet')
```
### # Writing Parquet files
```py
df.write.format('parquet').save('filename.parquet')

df.write.parquet('filename.parquet')
```
### # parquet sql
```py
fl1ght_df = sparK.read.parquet('flights.parquet')

flight_df.createOrReplaceTempView('flights')

short_flights_df = spark.sql('SELECT * FROM FLights WHERE flightduration < 100')
```
## Saving a DataFrame in Parquet format
- [x] View the row count of df1 and df2.
- [x] Combine df1 and df2 in a new DataFrame named df3 with the union method.
- [x] Save df3 to a parquet file named AA_DFW_ALL.parquet.
- [x] Read the AA_DFW_ALL.parquet file and show the count.
```py
# View the row count of df1 and df2
print("df1 Count: %d" % df1.count())
print("df2 Count: %d" % df2.count())

# Combine the DataFrames into one
df3 = df1.union(df2)

# Save the df3 DataFrame in Parquet format
df3.write.parquet('AA_DFW_ALL.parquet', mode='overwrite')

# Read the Parquet file into a new DataFrame and run a count
print(spark.read.parquet('AA_DFW_ALL.parquet').count())

''' result : 
df1 Count: 139359
df2 Count: 119911
259270
'''
```
## SQL and Parquet
- [x] Import the AA_DFW_ALL.parquet file into flights_df.
- [x] Use the createOrReplaceTempView method to alias the flights table.
- [x] Run a Spark SQL query against the flights table.
```py
# Read the Parquet file into flights_df
flights_df = spark.read.parquet('AA_DFW_ALL.parquet')

# Register the temp table
flights_df.createOrReplaceTempView('flights')

# Run a SQL query of the average flight duration
avg_duration = spark.sql(
    'SELECT avg(flight_duration) from flights').collect()[0]
print('The average flight time is: %d' % avg_duration)

# The average flight time is: 151
```
