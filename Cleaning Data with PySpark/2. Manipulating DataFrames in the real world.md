## # DataFrame column operations
### # example
```py
#Return rows where name starts with "M"
voter df. filter (voter_df.name.like ( M%*))

#Return name and position only
voters voter_df. select (' name", 'position ')
```
### # Common DataFrame transformations
negate with `~`
```py
# Filter/Where
voter_df.filter (voter_df.date > '1/1/2019') # or voter_df.where ( . . .)

# Select
Voter_df.select(voter_df.name)

# withColumn
voter_df.withColumn ('year', voter_df.date.year)

# drop
voter_df.drop("unused_colLumn")
```
### # Column string transformations
- Contained in pyspark.sql.functions
```py
import pyspark. sql. functions as F

# Applied per column as transformation
voter_df. withColumn ( "upper', F.upper (" name " ))

# Can create intermediary columns
voter_df.withColumn ( 'splits', F. split (' name", ))

# Can cast to other types
voter df.withcolumn('year ", voter_df ['_c4'].cast (IntegerType ()))
```
### # ArrayType) column functions
- Various utility functions / transformations to interact with ArrayType0
`.size(<column>)` returns length of arrayType) column
`.getItem(<index>)` used to retrieve a specific item at index of list column.
## Filtering column content with Python
- [x] Show the distinct VOTER_NAME entries.
- [x] Filter voter_df where the VOTER_NAME is 1-20 characters in length.
- [x] Filter out voter_df where the VOTER_NAME contains an _.
- [x] Show the distinct VOTER_NAME entries again.
```py
# pyspark.sql.functions as F
''' clean and take out large results or null results to get a cleaned table of names without '_' '''
# Show the distinct VOTER_NAME entries
voter_df.select('VOTER_NAME').distinct().show(40, truncate=False)

# Filter voter_df where the VOTER_NAME is 1-20 characters in length
voter_df = voter_df.filter(
    'length(VOTER_NAME) > 0 and length(VOTER_NAME) < 20')

# Filter out voter_df where the VOTER_NAME contains an underscore
voter_df = voter_df.filter(~ F.col('VOTER_NAME').contains('_'))

# Show the distinct VOTER_NAME entries again
voter_df.select('VOTER_NAME').distinct().show(40, truncate=False)
```
## Filtering Question #1
Consider the following Data Frame called users_df:

![image](https://user-images.githubusercontent.com/51888893/203785644-6b4e3f36-00b4-48bb-bfc0-e7efd1633113.png)

> If you wanted to return only the entries without nulls, which of following options would not work?

Answer the question
- [ ] users_df = users_df.filter(users_df.Name.isNotNull())
- [x] users_df = users_df.where(users_df.ID == 18502)
- [ ] users_df = users_df.where(~ (users_df.ID == 18502) )
- [ ] users_df = users_df.filter(~ col('Name').isNull())
## Filtering Question #2
Consider the following Data Frame called users_df:

![image](https://user-images.githubusercontent.com/51888893/203786237-0f72214b-e546-4b6f-9df5-ccbe400e9b40.png)

> If we wanted to return only the Name and State fields for any ID greater than 3000, which code snippet meets these requirements?

Answer the question
- [x] users_df.filter('ID > 3000').select("Name", "State")
- [ ] users_df.select("Name", "State").filter('ID > 3000')
- [ ] users_df.filter(users_df.ID = 3260).filter(users_df.ID = 18502)
- [ ] users_df.select("Name", "State")
## Modifying DataFrame columns
- [x] Add a new column called splits holding the list of possible names.
- [x] Use the getItem() method and create a new column called first_name.
- [x] Get the last entry of the splits list and create a column called last_name.
- [x] Drop the splits column and show the new voter_df.
```py
# pyspark.sql.functions as F

# Add a new column called splits separated on whitespace
voter_df = voter_df.withColumn('splits', F.split(voter_df.VOTER_NAME, '\s+')) # sql in whitespace

# Create a new column called first_name based on the first item in splits
''' get first item from VOTER_NAME in 'splits' '''
voter_df = voter_df.withColumn('first_name',  voter_df.splits.getItem(0)) 

''' get last item from VOTER_NAME of 'splits's size = (-1) '''
# Get the last entry of the splits list and create a column called last_name
voter_df = voter_df.withColumn('last_name', voter_df.splits.getItem(F.size('splits') - 1)) # sql

# Drop the splits column
voter_df = voter_df.drop('splits')

# Show the voter_df DataFrame
voter_df.show()
```
    +----------+-------------+-------------------+----------+---------+
    |      DATE|        TITLE|         VOTER_NAME|first_name|last_name|
    +----------+-------------+-------------------+----------+---------+
    |02/08/2017|Councilmember|  Jennifer S. Gates|  Jennifer|    Gates|
    |02/08/2017|Councilmember| Philip T. Kingston|    Philip| Kingston|
## # Conditional DataFrame column operations
### # Conditional clauses
Conditional Clauses are:
- Inline version of :  if/ then / else
- `when()`
- `otherwise()`
when (<if condition>, <then x>)
```py
df.select (df. Name, df.Age,
    .when (df.Age >= 18, "Adult")
    .otherwise ("Minor"))
```
![image](https://user-images.githubusercontent.com/51888893/203795676-74f67ed4-05a0-497a-825b-77e060ebd2fd.png)

## when() example
- [x] Add a column to voter_df named random_val with the results of the F.rand() method for any voter with the title Councilmember.
- [x] Show some of the DataFrame rows, noting whether the .when() clause worked.
```py
# pyspark.sql.functions library is available as F.

# Add a column to voter_df for any voter with the title **Councilmember**
voter_df = voter_df.withColumn('random_val',
                               when(voter_df.TITLE == 'Councilmember', F.rand()))

# Show some of the DataFrame rows, noting whether the when clause worked
voter_df.show()
```
        +----------+-------------+-------------------+-------------------+
        |      DATE|        TITLE|         VOTER_NAME|         random_val|
        +----------+-------------+-------------------+-------------------+
        |02/08/2017|Councilmember|  Jennifer S. Gates| 0.8646982973286065|
        |02/08/2017|Councilmember| Philip T. Kingston| 0.3836413143814139|
        |02/08/2017|        Mayor|Michael S. Rawlings|               null|
## When / Otherwise
- [x]  Add a column to voter_df named random_val with the results of the F.rand() method for any voter with the title Councilmember. Set random_val to 2 for the - [x]  - [x]  Mayor. Set any other title to the value 0.
- [x]  Show some of the Data Frame rows, noting whether the clauses worked.
- [x]  Use the .filter clause to find 0 in random_val.
```py
# Add a column to voter_df for a voter based on their position
voter_df = voter_df.withColumn('random_val',
                               when(voter_df.TITLE == 'Councilmember', F.rand())
                               .when(voter_df.TITLE == 'Mayor', 2)
                               .otherwise(0)
                              )

# Show some of the DataFrame rows
voter_df.show()

# Use the .filter() clause with random_val
voter_df.filter(voter_df.random_val == 0).show()
```
## # User defined functions
`pyspark.sql.functions.udf`
```py
# Define a Python method
def reverseString (mystr) :
return mystr[::-1]
    
# Wrap the function and store as a variable
udfReverseString = udf(reverseString, StringType ())
    
# Use with Spark
user_df = user_df.withColumn ('ReverseName',
udfReverseString (user_df.Name))
```
```py
def sortingCap():
    return random. choice ([ 'G', 'H', R', 'S' ])
    
# Wrap the function and store as a variable
udfSortingCap = udf (sortingCap, StringType ())

# Use with Spark
user_df = user_df.withColumn ( 'Class' , udfSortingCap ())
```
## Understanding user defined functions
> When creating a new user defined function, which is not a possible value for the second argument?

Answer the question
- [ ] ArrayType(IntegerType())
- [ ] IntegerType()
- [ ] LongType()
- [x] udf()
- [ ] StringType()
## Using user defined functions in Spark
- [x] Edit the getFirstAndMiddle() function to return a space separated string of names, except the last entry in the names list.
- [x] Define the function as a user-defined function. It should return a string type.
- [x] Create a new column on voter_df called first_and_middle_name using your UDF.
- [x] Show the Data Frame.
```py
def getFirstAndMiddle(names):
  # Return a space separated string of names
  return ' '.join(names[:-1]) # not last name

# Define the method as a UDF
udfFirstAndMiddle = F.udf(getFirstAndMiddle, StringType()) 
''' udf are vars applicable to tables '''

# Create a new column using your UDF
voter_df = voter_df.withColumn('first_and_middle_name', udfFirstAndMiddle(voter_df.splits))

# Show the DataFrame
voter_df.show()
```
## Adding an ID Field
- [x] Select the unique entries from the column VOTER NAME and create a new DataFrame called voter_df.
- [x] Count the rows in the voter_df DataFrame.
- [x] Add a ROW_ID column using the appropriate Spark function.
- [x] Show the rows with the 10 highest ROW_IDs.
```py
# Select all the unique council voters
voter_df = df.select(df["VOTER NAME"]).distinct()

# Count the rows in voter_df
print("\nThere are %d rows in the voter_df DataFrame.\n" % voter_df.count())

# Add a ROW_ID
voter_df = voter_df.withColumn('ROW_ID', F.monotonically_increasing_id())

# Show the rows with 10 highest IDs in the set
voter_df.orderBy(voter_df.ROW_ID.desc()).show(10)
```
    here are 36 rows in the voter_df DataFrame.

    +--------------------+------+
    |          VOTER NAME|ROW_ID|
    +--------------------+------+
    |        Lee Kleinman|    35|
    |  the  final  201...|    34|
    |         Erik Wilson|    33|
    |  the  final   20...|    32|
    | Carolyn King Arnold|    31|
    | Rickey D.  Callahan|    30|
    |   the   final  2...|    29|
    |    Monica R. Alonzo|    28|
    |     Lee M. Kleinman|    27|
    |   Jennifer S. Gates|    26|
    +--------------------+------+
## IDs with different partitions
- [x] Print the number of partitions on each DataFrame.
- [x] Add a ROW_ID field to each DataFrame.
- [x] Show the top 10 IDs in each DataFrame.
```py
# Print the number of partitions in each DataFrame
print("\nThere are %d partitions in the voter_df DataFrame.\n" % voter_df.rdd.getNumPartitions())
print("\nThere are %d partitions in the voter_df_single DataFrame.\n" % voter_df_single.rdd.getNumPartitions())

# Add a ROW_ID field to each DataFrame
voter_df = voter_df.withColumn('ROW_ID', F.monotonically_increasing_id())
voter_df_single = voter_df_single.withColumn('ROW_ID', F.monotonically_increasing_id())

# Show the top 10 IDs in each DataFrame 
voter_df.orderBy(voter_df.ROW_ID.desc()).show(10)
voter_df_single.orderBy(voter_df_single.ROW_ID.desc()).show(10)
```
## More ID tricks
- [x] Determine the highest ROW_ID in voter_df_march and save it in the variable previous_max_ID. The statement .rdd.max()[0] will get the maximum ID.
- [x] Add a ROW_ID column to voter_df_april starting at the value of previous_max_ID.
- [x] Show the ROW_ID's from both Data Frames and compare.
```py
# Determine the highest ROW_ID and save it in previous_max_ID
previous_max_ID = voter_df_march.select('ROW_ID').rdd.max()[0]

# Add a ROW_ID column to voter_df_april starting at the desired value
voter_df_april = voter_df_april.withColumn(
    'ROW_ID', F.monotonically_increasing_id() + previous_max_ID)

# Show the ROW_ID from both DataFrames and compare
voter_df_march.select('ROW_ID').show()
voter_df_april.select('ROW_ID').show()
```
