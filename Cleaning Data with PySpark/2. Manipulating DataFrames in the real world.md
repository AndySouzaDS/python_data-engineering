## # DataFrame column operations
### # example
```py
#Return rows where name starts with "M"
voter df. filter (voter_df.name.like ( M%*))

#Return name and position only
voters voter_df. select (' name", 'position ')
```
### # Common DataFrame transformations
negate with `~`
```py
# Filter/Where
voter_df.filter (voter_df.date > '1/1/2019') # or voter_df.where ( . . .)

# Select
Voter_df.select(voter_df.name)

# withColumn
voter_df.withColumn ('year', voter_df.date.year)

# drop
voter_df.drop("unused_colLumn")
```
### # Column string transformations
- Contained in pyspark.sql.functions
```py
import pyspark. sql. functions as F

# Applied per column as transformation
voter_df. withColumn ( "upper', F.upper (" name " ))

# Can create intermediary columns
voter_df.withColumn ( 'splits', F. split (' name", ))

# Can cast to other types
voter df.withcolumn('year ", voter_df ['_c4'].cast (IntegerType ()))
```
### # ArrayType) column functions
- Various utility functions / transformations to interact with ArrayType0
`.size(<column>)` returns length of arrayType) column
`.getItem(<index>)` used to retrieve a specific item at index of list column.
## Filtering column content with Python
- [x] Show the distinct VOTER_NAME entries.
- [x] Filter voter_df where the VOTER_NAME is 1-20 characters in length.
- [x] Filter out voter_df where the VOTER_NAME contains an _.
- [x] Show the distinct VOTER_NAME entries again.
```py
# pyspark.sql.functions as F
''' clean and take out large results or null results to get a cleaned table of names without '_' '''
# Show the distinct VOTER_NAME entries
voter_df.select('VOTER_NAME').distinct().show(40, truncate=False)

# Filter voter_df where the VOTER_NAME is 1-20 characters in length
voter_df = voter_df.filter(
    'length(VOTER_NAME) > 0 and length(VOTER_NAME) < 20')

# Filter out voter_df where the VOTER_NAME contains an underscore
voter_df = voter_df.filter(~ F.col('VOTER_NAME').contains('_'))

# Show the distinct VOTER_NAME entries again
voter_df.select('VOTER_NAME').distinct().show(40, truncate=False)
```
## Filtering Question #1
Consider the following Data Frame called users_df:

![image](https://user-images.githubusercontent.com/51888893/203785644-6b4e3f36-00b4-48bb-bfc0-e7efd1633113.png)

> If you wanted to return only the entries without nulls, which of following options would not work?

Answer the question
- [ ] users_df = users_df.filter(users_df.Name.isNotNull())
- [x] users_df = users_df.where(users_df.ID == 18502)
- [ ] users_df = users_df.where(~ (users_df.ID == 18502) )
- [ ] users_df = users_df.filter(~ col('Name').isNull())
## Filtering Question #2
Consider the following Data Frame called users_df:

![image](https://user-images.githubusercontent.com/51888893/203786237-0f72214b-e546-4b6f-9df5-ccbe400e9b40.png)

> If we wanted to return only the Name and State fields for any ID greater than 3000, which code snippet meets these requirements?

Answer the question
- [x] users_df.filter('ID > 3000').select("Name", "State")
- [ ] users_df.select("Name", "State").filter('ID > 3000')
- [ ] users_df.filter(users_df.ID = 3260).filter(users_df.ID = 18502)
- [ ] users_df.select("Name", "State")
## Modifying DataFrame columns
- [x] Add a new column called splits holding the list of possible names.
- [x] Use the getItem() method and create a new column called first_name.
- [x] Get the last entry of the splits list and create a column called last_name.
- [x] Drop the splits column and show the new voter_df.
```py
# pyspark.sql.functions as F

# Add a new column called splits separated on whitespace
voter_df = voter_df.withColumn('splits', F.split(voter_df.VOTER_NAME, '\s+')) # sql in whitespace

# Create a new column called first_name based on the first item in splits
''' get first item from VOTER_NAME in 'splits' '''
voter_df = voter_df.withColumn('first_name',  voter_df.splits.getItem(0)) 

''' get last item from VOTER_NAME of 'splits's size = (-1) '''
# Get the last entry of the splits list and create a column called last_name
voter_df = voter_df.withColumn('last_name', voter_df.splits.getItem(F.size('splits') - 1)) # sql

# Drop the splits column
voter_df = voter_df.drop('splits')

# Show the voter_df DataFrame
voter_df.show()
```
    +----------+-------------+-------------------+----------+---------+
    |      DATE|        TITLE|         VOTER_NAME|first_name|last_name|
    +----------+-------------+-------------------+----------+---------+
    |02/08/2017|Councilmember|  Jennifer S. Gates|  Jennifer|    Gates|
    |02/08/2017|Councilmember| Philip T. Kingston|    Philip| Kingston|
## # Conditional DataFrame column operations
### # Conditional clauses
Conditional Clauses are:
- Inline version of :  if/ then / else
- `when()`
- `otherwise()`
when (<if condition>, <then x>)
```py
df.select (df. Name, df.Age,
    .when (df.Age >= 18, "Adult")
    .otherwise ("Minor"))
```
![image](https://user-images.githubusercontent.com/51888893/203795676-74f67ed4-05a0-497a-825b-77e060ebd2fd.png)

## when() example
- [x] Add a column to voter_df named random_val with the results of the F.rand() method for any voter with the title Councilmember.
- [x] Show some of the DataFrame rows, noting whether the .when() clause worked.
```py
# pyspark.sql.functions library is available as F.

# Add a column to voter_df for any voter with the title **Councilmember**
voter_df = voter_df.withColumn('random_val',
                               when(voter_df.TITLE == 'Councilmember', F.rand()))

# Show some of the DataFrame rows, noting whether the when clause worked
voter_df.show()
```
        +----------+-------------+-------------------+-------------------+
        |      DATE|        TITLE|         VOTER_NAME|         random_val|
        +----------+-------------+-------------------+-------------------+
        |02/08/2017|Councilmember|  Jennifer S. Gates| 0.8646982973286065|
        |02/08/2017|Councilmember| Philip T. Kingston| 0.3836413143814139|
        |02/08/2017|        Mayor|Michael S. Rawlings|               null|
## When / Otherwise
- [x]  Add a column to voter_df named random_val with the results of the F.rand() method for any voter with the title Councilmember. Set random_val to 2 for the - [x]  - [x]  Mayor. Set any other title to the value 0.
- [x]  Show some of the Data Frame rows, noting whether the clauses worked.
- [x]  Use the .filter clause to find 0 in random_val.
```py
# Add a column to voter_df for a voter based on their position
voter_df = voter_df.withColumn('random_val',
                               when(voter_df.TITLE == 'Councilmember', F.rand())
                               .when(voter_df.TITLE == 'Mayor', 2)
                               .otherwise(0)
                              )

# Show some of the DataFrame rows
voter_df.show()

# Use the .filter() clause with random_val
voter_df.filter(voter_df.random_val == 0).show()
```
## # User defined functions
`pyspark.sql.functions.udf`
```py
# Define a Python method
def reverseString (mystr) :
return mystr[::-1]
    
# Wrap the function and store as a variable
udfReverseString = udf(reverseString, StringType ())
    
# Use with Spark
user_df = user_df.withColumn ('ReverseName',
udfReverseString (user_df.Name))
```
