## # Caching
call`.cache ()` on the DataFrame before Action
```py
voter_df spark.read.csv("voter_data.txt.gz')
voter_df.cache ().count ()

voter_df = voter_df. withColumn ("IDÂ°, monotonically_increasing_id ())
voter_df = voter_df.cache (O
voter_df.show()
```
### # more chache operators
Check `.is_cached` to determine cache status
```py
print (voter_df.is_cached)
```
    True
Call `unpersist ()` when finished with DataFrame
## Caching a DataFrame
- [x] Cache the unique rows in the departures_df DataFrame.
- [x] Perform a count query on departures_df, noting how long the operation takes.
- [x] Count the rows again, noting the variance in time of a cached DataFrame.
```py
start_time = time.time()

# Add caching to the unique rows in departures_df
departures_df = departures_df.distinct().cache()

# Count the unique rows in departures_df, noting how long the operation takes
print("Counting %d rows took %f seconds" %
      (departures_df.count(), time.time() - start_time))

# Count the rows again, noting the variance in time of a cached DataFrame
start_time = time.time()
print("Counting %d rows again took %f seconds" %
      (departures_df.count(), time.time() - start_time))
```
## Removing a DataFrame from cache
- [x] Check the caching status on the departures_df DataFrame.
- [x] Remove the departures_df DataFrame from the cache.
- [x] Validate the caching status again.
```py
