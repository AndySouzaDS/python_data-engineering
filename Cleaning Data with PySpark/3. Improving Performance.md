## # Caching
call`.cache ()` on the DataFrame before Action
```py
voter_df spark.read.csv("voter_data.txt.gz')
voter_df.cache ().count ()

voter_df = voter_df. withColumn ("IDÂ°, monotonically_increasing_id ())
voter_df = voter_df.cache (O
voter_df.show()
```
### # more chache operators
Check `.is_cached` to determine cache status
```py
print (voter_df.is_cached)
```
    True
Call `unpersist ()` when finished with DataFrame
## Caching a DataFrame
- [x] Cache the unique rows in the departures_df DataFrame.
- [x] Perform a count query on departures_df, noting how long the operation takes.
- [x] Count the rows again, noting the variance in time of a cached DataFrame.
```py
start_time = time.time()

# Add caching to the unique rows in departures_df
departures_df = departures_df.distinct().cache()

# Count the unique rows in departures_df, noting how long the operation takes
print("Counting %d rows took %f seconds" %
      (departures_df.count(), time.time() - start_time))

# Count the rows again, noting the variance in time of a cached DataFrame
start_time = time.time()
print("Counting %d rows again took %f seconds" %
      (departures_df.count(), time.time() - start_time))
```
## Removing a DataFrame from cache
- [x] Check the caching status on the departures_df DataFrame.
- [x] Remove the departures_df DataFrame from the cache.
- [x] Validate the caching status again.
```py
# Determine if departures_df is in the cache
print("Is departures_df cached?: %s" % departures_df.is_cached)
print("Removing departures_df from cache")

# Remove departures_df from the cache
departures_df.unpersist()

# Check the cache status again
print("Is departures_df cached?: %s" % departures_df.is_cached)

'''
Is departures_df cached?: True
Removing departures_df from cache
Is departures_df cached?: False
'''
```
## # Improve import performance
`import performance`
- Can import via wildcard
```py
airport.df = spark.read.CSV('airports-*.txt.gz')
```
### # How to split objects
- Use OS utilities / scripts `(split, cut, awk)`
```py
split -l 10000 -d largefile chunk-
```
- Use custom scripts
- Write out to Parquet
```py
df_csv = spark.read.csv('singlelargefile.csv')
df_csv.Write.parquet("data.parquet")
df = spark.read.parquet("data.parquet")
```
