## # Caching
call`.cache ()` on the DataFrame before Action
```py
voter_df spark.read.csv("voter_data.txt.gz')
voter_df.cache ().count ()

voter_df = voter_df. withColumn ("IDÂ°, monotonically_increasing_id ())
voter_df = voter_df.cache (O
voter_df.show()
```
### # more chache operators
Check `.is_cached` to determine cache status
```py
print (voter_df.is_cached)
```
    True
Call `unpersist ()` when finished with DataFrame
## Caching a DataFrame
- [x] Cache the unique rows in the departures_df DataFrame.
- [x] Perform a count query on departures_df, noting how long the operation takes.
- [x] Count the rows again, noting the variance in time of a cached DataFrame.
```py
start_time = time.time()

# Add caching to the unique rows in departures_df
departures_df = departures_df.distinct().cache()

# Count the unique rows in departures_df, noting how long the operation takes
print("Counting %d rows took %f seconds" %
      (departures_df.count(), time.time() - start_time))

# Count the rows again, noting the variance in time of a cached DataFrame
start_time = time.time()
print("Counting %d rows again took %f seconds" %
      (departures_df.count(), time.time() - start_time))
```
## Removing a DataFrame from cache
- [x] Check the caching status on the departures_df DataFrame.
- [x] Remove the departures_df DataFrame from the cache.
- [x] Validate the caching status again.
```py
# Determine if departures_df is in the cache
print("Is departures_df cached?: %s" % departures_df.is_cached)
print("Removing departures_df from cache")

# Remove departures_df from the cache
departures_df.unpersist()

# Check the cache status again
print("Is departures_df cached?: %s" % departures_df.is_cached)

'''
Is departures_df cached?: True
Removing departures_df from cache
Is departures_df cached?: False
'''
```
## # Improve import performance
`import performance`
- Can import via wildcard
```py
airport.df = spark.read.CSV('airports-*.txt.gz')
```
### # How to split objects
- Use OS utilities / scripts `(split, cut, awk)`
```py
split -l 10000 -d largefile chunk-
```
- Use custom scripts
- Write out to Parquet
```py
df_csv = spark.read.csv('singlelargefile.csv')
df_csv.Write.parquet("data.parquet")
df = spark.read.parquet("data.parquet")
```
## File size optimization
Consider if you're given 2 large data files on a cluster with 10 nodes. Each file contains 10M rows of roughly the same size. While working with your data, the responsiveness is acceptable but the initial read from the files takes a considerable period of time. Note that you are the only one who will use the data and it changes for each run.

> Which of the following is the best option to improve performance?

Answer the question
- [ ] Split the 2 files into 8 files of 2.5M rows each.
- [ ] Convert the files to parquet.
- [x] Split the 2 files into 50 files of 400K rows each.
- [ ] Split the files into 30 files containing a random number of rows.
## File import performance
- [x] Import the departures_full.txt.gz file and the departures_xxx.txt.gz files into separate DataFrames.
- [x] Run a count on each DataFrame and compare the run times.
```py
# Import the full and split files into DataFrames
full_df = spark.read.csv('departures_full.txt.gz')
split_df = spark.read.csv('departures_*.txt.gz')

# Print the count and run time for each DataFrame
start_time_a = time.time()
print("Total rows in full DataFrame:\t%d" % full_df.count())
print("Time to run: %f" % (time.time() - start_time_a))

start_time_b = time.time()
print("Total rows in split DataFrame:\t%d" % split_df.count())
print("Time to run: %f" % (time.time() - start_time_b))

'''
Total rows in full DataFrame:	139359
Time to run: 0.314305
Total rows in split DataFrame:	278718
Time to run: 0.569276
'''
```
## # cluster configurations
```py
# Reading configuration settings:
spark.conf.get (<configuration name>)
    
# Writing configuration settings
spark.conf.set(<configuration name>)
```
### # Cluster Types
Spark deployment options:
- Single node
- Standalone
- Managed
   - YARN
   - Mesobs
   - Kubernetes
## Reading Spark configurations
- [x] Check the name of the Spark application instance ('spark.app.name').
- [x] Determine the TCP port the driver runs on ('spark.driver.port').
- [x] Determine how many partitions are configured for joins.
- [x] Show the results.
```py
# Name of the Spark application instance
app_name = spark.conf.get('spark.app.name')

# Driver TCP port
driver_tcp_port = spark.conf.get('spark.driver.port')

# Number of join partitions
num_partitions = spark.conf.get('spark.sql.shuffle.partitions')

# Show the results
print("Name: %s" % app_name)
print("Driver TCP port: %s" % driver_tcp_port)
print("Number of partitions: %s" % num_partitions)

'''
Name: pyspark-shell
Driver TCP port: 33817
Number of partitions: 200
'''
```
## Writing Spark configurations
- [x] Store the number of partitions in departures_df in the variable before.
- [x] Change the spark.sql.shuffle.partitions configuration to 500 partitions.
- [x] Recreate the departures_df DataFrame reading the distinct rows from the departures file.
- [x] Print the number of partitions from before and after the configuration change.
```py
# Store the number of partitions in variable
before = departures_df.rdd.getNumPartitions()

# Configure Spark to use 500 partitions
spark.conf.set('spark.sql.shuffle.partitions', 500)

# Recreate the DataFrame using the departures data file
departures_df = spark.read.csv('departures.txt.gz').distinct()

# Print the number of partitions for each instance
print("Partition count before change: %d" % before)
print("Partition count after change: %d" %
      departures_df.rdd.getNumPartitions())

''' 
Partition count before change: 1
Partition count after change: 1
'''
```
## # Performance improvements
`view plan`
```py
voter_df.explain()
```
### # How to limit shufling?
- Limit use of `.repartition (num_partitions)`
- Use `.coalesce (num_partitions)` instead
Use care when caling .join()
- Use `broadcast ()`
May not need to limit it
### # Broadcasting
`Broadcasting` : Provides a copy of an object to each worker
- Prevents undue / excess communication between nodes
- Can drastically speed up `join ()` operations
- Use the `.broadcast (<DataFrame>) method
```py
from pyspark. sql.functions import broadcast
combineddf = df_1.join (broadcast (df_2))
```
