## Quick pipeline
- [x] Import the file 2015-departures.csv.gz to a DataFrame. Note the header is already defined.
- [x] Filter the DataFrame to contain only flights with a duration over 0 minutes. Use the index of the column, not the column name (remember to use .printSchema() to see the column names / order).
- [x] Add an ID column.
- [x] Write the file out as a JSON document named output.json.
```py
# Import the data to a DataFrame
departures_df = spark.read.csv('2015-departures.csv.gz', header=True)

# Remove any duration of 0
departures_df = departures_df.filter(
     departures_df['Actual elapsed time (Minutes)'] != 0
)

# Add an ID column
departures_df = departures_df.withColumn('id',F.monotonically_increasing_id())

# Write the file out to JSON format
departures_df.write.json('output.json', mode='overwrite')
```
## Pipeline data issue
![image](https://user-images.githubusercontent.com/51888893/203979714-d15aa806-d675-4665-bdc2-0e80d7c40c16.png)

there's a problem in the dataset while trying to sort the duration data. She's not sure what the issue is beyond the sorting operation not working as expected.
> After analyzing the data, which command would fix the issue?
- [ ] departures_df = departures_df.orderBy(departures_df.Airport)
- [x] departures_df = departures_df.withColumn('Duration', departures_df['Duration'].cast(IntegerType()))
- [ ] departures_df = departures_df.orderBy(departures_df['Duration'])
- [ ] departures_df = departures_df.select(departures_df['Duration']).cast(LongType())

## # Data handling techniques
### # Removing blank lines, headers, and comments
`Spark's CSV parser:`
```py
# remove comments 'comment'
spark.read.csv("datafile . cSv.gz', comment= '#')

# Remove 'header'
df1 spark.read.csv("datatile.csv.gz', header="False"

# automatic column creation 'sep'
spark.read.csv( 'datafile.csv.gz', sep=',')
```
stores data in column defaulting to `_c0`
## Removing commented lines
- [x] Import the annotations.csv.gz file to a DataFrame and perform a row count. Specify a separator character of |.
- [x] Query the data for the number of rows beginning with #.
- [x] Import the file again to a new DataFrame, but specify the comment character in the options to remove any commented rows.
- [x] Count the new DataFrame and verify the difference is as expected.
```py
# Import the file to a DataFrame and perform a row count
annotations_df = spark.read.csv('annotations.csv.gz', sep='|')
full_count = annotations_df.count()

# Count the number of rows beginning with '#'
comment_count = annotations_df.filter(col('_c0').startswith('#')).count()

# Import the file to a new DataFrame, without commented rows
no_comments_df = spark.read.csv('annotations.csv.gz', sep='|', comment='#')

# Count the new DataFrame and verify the difference is as expected
no_comments_count = no_comments_df.count()
print("Full count: %d\nComment count: %d\nRemaining count: %d" % (full_count, comment_count, no_comments_count))

'''
Full count: 32794
Comment count: 1416
Remaining count: 31378
'''
```
## Removing invalid rows
- [x] Create a new variable tmp_fields using the annotations_df DataFrame column '_c0' splitting it on the tab character.
- [x] Create a new column in annotations_df named 'colcount' representing the number of fields defined in the previous step.
- [x] Filter out any rows from annotations_df containing fewer than 5 fields.
```py
# Split _c0 on the tab character and store the list in a variable
tmp_fields = F.split(annotations_df['_c0'], '\t')

# Create the colcount column on the DataFrame
annotations_df = annotations_df.withColumn('colcount', F.size(tmp_fields))

# Remove any rows containing fewer than 5 fields
annotations_df_filtered = annotations_df.filter(
    ~ (annotations_df['colcount'] > 4))

# Count the number of rows
final_count = annotations_df_filtered.count()
print("Initial count: %d\nFinal count: %d" % (initial_count, final_count))

'''
Initial count: 31378
Final count: 10798
'''
```

## Splitting into columns
- [x] Split the content of the '_c0' column on the tab character and store in a variable called split_cols.
- [x] Add the following columns based on the first four entries in the variable above: folder, filename, width, height on a DataFrame named split_df.
- [x] Add the split_cols variable as a column.
```py
# Split the content of _c0 on the tab character (aka, '\t')
split_cols = F.split(annotations_df['_c0'], '\t')

# Add the columns folder, filename, width, and height
split_df = annotations_df.withColumn('folder', split_cols.getItem(0))
split_df = split_df.withColumn('filename', split_cols.getItem(1))
split_df = split_df.withColumn('width', split_cols.getItem(2))
split_df = split_df.withColumn('height', split_cols.getItem(3))

# Add split_cols as a column
split_df = split_df.withColumn('split_cols', split_cols)
```
## Further parsing
- [x] Create a new function called retriever that takes two arguments, the split columns (cols) and the total number of columns (colcount). This function should return a list of the entries that have not been defined as columns yet (i.e., everything after item 4 in the list).
- [x] Define the function as a Spark UDF, returning an Array of strings.
- [x] Create the new column dog_list using the UDF and the available columns in the DataFrame.
- [x] Remove the columns _c0, colcount, and split_cols.
```py
def retriever(cols, colcount):
    # Return a list of dog data
    return cols[4:colcount]

# Define the method as a UDF , to define SQL metrod for calculations in df
udfRetriever = F.udf(retriever, ArrayType(StringType()))

# Create a new column using your UDF
split_df = split_df.withColumn('dog_list', udfRetriever(
    split_df['split_cols'], split_df['colcount']))

# Remove the original column, split_cols, and the colcount
split_df = split_df.drop('_c0').drop('colcount').drop('split_cols')
```
## # Data validation
### # Validating via joins
- Compares data against known values
- Easy to find data in a given set
```py
parsed_df = spark.read.parquet ("parsed_data. parquet ')
company_df = spark.read. parquet (" companies . parquet ')
verified_df = parsed_df. join (company_df, parsed_df.company = company_df.company)
```
This automatically removes any rows with a company not in the `valid_df`!

## Validate rows via join
- [x] Rename the _c0 column to folder on the valid_folders_df DataFrame.
- [x] Count the number of rows in split_df.
- [x] Join the two DataFrames on the folder name, and call the resulting DataFrame joined_df. Make sure to broadcast the smaller DataFrame.
- [x] Check the number of rows remaining in the DataFrame and compare
```py
# Rename the column in valid_folders_df
valid_folders_df = valid_folders_df.withColumnRenamed('_c0','folder')

# Count the number of rows in split_df
split_count = split_df.count()

# Join the DataFrames on the folder name
joined_df = split_df.join(F.broadcast(valid_folders_df), "folder")

# Compare the number of rows remaining
joined_count = joined_df.count()
print("Before: %d\nAfter: %d" % (split_count, joined_count))

'''
Before: 20580
After: 19956
'''
```
`When should I broadcast join Spark?` :

when we want to join one small data frame with the large one. the requirement here is we should be able to store the small data frame easily in the memory so that we can join them with the large data frame in order to boost the performance of the join

## Examining invalid rows
- [x] Determine the row counts for each DataFrame.
- [x] Create a DataFrame containing only the invalid rows.
- [x] Validate the count of the new DataFrame is as expected.
- [x] Determine the number of distinct folder rows removed.
```py
